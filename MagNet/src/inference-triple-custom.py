"""
ä½¿ç”¨ä¸‰è”åŠ¨æ¨¡å‹ç”Ÿæˆå¿ƒç†å’¨è¯¢å¯¹è¯

ç‰¹ç‚¹ï¼š
- ä¿ç•™ Client Agent çš„åŸæœ‰é€»è¾‘ï¼ˆä½¿ç”¨æ¡†æ¶é¢„è®¾æç¤ºè¯ï¼‰
- ä½¿ç”¨ä¸‰è”åŠ¨æ¨¡å‹ï¼ˆæƒ…æ„Ÿåˆ†æ+é¡¾é—®æ¨¡å‹+ä¸»æ¨¡å‹ï¼‰ä½œä¸º Counselor Agent
- æ”¯æŒè‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
- ç”Ÿæˆä¸åŸå§‹æ ¼å¼ä¸€è‡´çš„ session_*.json æ–‡ä»¶

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # ä½¿ç”¨é…ç½®æ–‡ä»¶
    python inference-triple-custom.py --config ../config_triple.json

    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    python inference-triple-custom.py --sentiment_model /path/to/sentiment.bin --primary_model /path/to/primary --consultant_model /path/to/consultant
"""

import argparse
import json
import multiprocessing
import traceback
import os
from pathlib import Path


class PromptTemplate:
    """ç®€å•çš„æç¤ºè¯æ¨¡æ¿ç±»ï¼ˆæ›¿ä»£ langchain.prompts.PromptTemplateï¼‰"""

    def __init__(self, input_variables, template):
        self.input_variables = input_variables
        self.template = template

    def format(self, **kwargs):
        result = self.template
        for var in self.input_variables:
            if var in kwargs:
                result = result.replace(f"{{{var}}}", str(kwargs[var]))
        return result


# å¯¼å…¥é€šç”¨ LLM å®¢æˆ·ç«¯ï¼ˆç”¨äº Client Agentï¼‰
from llm_client import create_client_from_env

# å¯¼å…¥ä¸‰è”åŠ¨æ¨¡å‹ Agent
from triple_model_agent import TripleModelAgent, get_triple_model_preset_prompt


# ============================================
# é…ç½®
# ============================================
DATA_FILE = "../dataset/data_cn.json"
PROMPTS_DIR = "../prompts/cn/"

# ç”¨äº Client Agent çš„ LLM å®¢æˆ·ç«¯ï¼ˆå…¨å±€å˜é‡ï¼Œå»¶è¿ŸåŠ è½½ï¼‰
client_llm = None


def load_env_file():
    """åŠ è½½ .env æ–‡ä»¶"""
    env_file = Path(__file__).parent.parent / ".env"
    if env_file.exists():
        print(f"åŠ è½½ç¯å¢ƒå˜é‡: {env_file}")
        with open(env_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    os.environ[key] = value
        print("ç¯å¢ƒå˜é‡åŠ è½½å®Œæˆ")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼ŒClient Agent å°†æ— æ³•å·¥ä½œ")


def generate_with_api(prompt: str) -> str:
    """ä½¿ç”¨ API ç”Ÿæˆå“åº”ï¼ˆç”¨äº Client Agentï¼‰"""
    global client_llm
    if client_llm is None:
        try:
            client_llm = create_client_from_env()
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•åˆ›å»º LLM å®¢æˆ·ç«¯: {e}")
            print("è¯·ç¡®ä¿ .env æ–‡ä»¶é…ç½®æ­£ç¡®")
            raise
    response = client_llm.completion(prompt=prompt)
    return response.choices[0].message.content


# ============================================
# åŸæœ‰ Client Agentï¼ˆä¿æŒä¸å˜ï¼‰
# ============================================
class ClientAgent:
    """æ¥è®¿è€…æ™ºèƒ½ä½“ï¼ˆä½¿ç”¨æ¡†æ¶åŸæœ‰é€»è¾‘å’Œæç¤ºè¯ï¼‰"""

    def __init__(self, example):
        self.example = example
        self._load_prompt()

    def _load_prompt(self):
        """åŠ è½½ Client Agent æç¤ºè¯"""
        prompt_path = Path(PROMPTS_DIR) / "agent_client.txt"
        if not prompt_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ° Client Agent æç¤ºè¯æ–‡ä»¶: {prompt_path}")

        with open(prompt_path, "r", encoding="utf-8") as f:
            prompt_text = f.read()

        self.attitude = (
            f"{self.example['AI_client']['attitude']}: "
            f"{self.example['AI_client']['attitude_instruction']}"
        )
        self.prompt_template = PromptTemplate(
            input_variables=["intake_form", "attitude", "history"],
            template=prompt_text
        )

    def generate(self, history):
        """ç”Ÿæˆæ¥è®¿è€…å“åº”"""
        history_text = '\n'.join([
            f"{message['role'].capitalize()}: {message['message']}"
            for message in history
        ])

        prompt = self.prompt_template.format(
            intake_form=self.example,
            attitude=self.attitude,
            history=history_text
        )

        return generate_with_api(prompt)


# ============================================
# ä¸‰è”åŠ¨æ¨¡å‹å’¨è¯¢ä¼šè¯
# ============================================
class TripleModelTherapySession:
    """ä½¿ç”¨ä¸‰è”åŠ¨æ¨¡å‹çš„å’¨è¯¢ä¼šè¯"""

    def __init__(
        self,
        example,
        max_turns: int,
        sentiment_model_path: str,
        primary_model_path: str,
        consultant_model_path: str,
        system_prompt: str = None,
        max_new_tokens: int = 2048,
        temperature: float = 0.5,
        n_sentiment_classes: int = 2
    ):
        """
        åˆå§‹åŒ–ä¼šè¯

        Args:
            example: æ•°æ®æ ·æœ¬
            max_turns: æœ€å¤§å¯¹è¯è½®æ•°
            sentiment_model_path: æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹è·¯å¾„
            primary_model_path: ä¸»æ¨¡å‹è·¯å¾„
            consultant_model_path: é¡¾é—®æ¨¡å‹è·¯å¾„
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            n_sentiment_classes: æƒ…æ„Ÿåˆ†ç±»æ•°é‡ (2: æ¶ˆæ/ç§¯æ, 3: æ¶ˆæ/ä¸­æ€§/ç§¯æ)
        """
        self.example = example
        self.max_turns = max_turns
        self.history = []

        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        self.client_agent = ClientAgent(example=example)
        self.counselor_agent = TripleModelAgent(
            example=example,
            sentiment_model_path=sentiment_model_path,
            primary_model_path=primary_model_path,
            consultant_model_path=consultant_model_path,
            system_prompt=system_prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            n_sentiment_classes=n_sentiment_classes
        )

    def _add_to_history(self, role: str, message: str):
        """æ·»åŠ æ¶ˆæ¯åˆ°å†å²"""
        self.history.append({"role": role, "message": message})

    def _initialize_session(self):
        """åˆå§‹åŒ–ä¼šè¯"""
        example_cbt = self.example['AI_counselor']['CBT']
        self._add_to_history("counselor", example_cbt['init_history_counselor'])
        self._add_to_history("client", example_cbt['init_history_client'])

    def _exchange_statements(self):
        """äº¤æ›¿ç”Ÿæˆå¯¹è¯"""
        for turn in range(self.max_turns):
            print(f"\n    è½®æ¬¡ {turn + 1}/{self.max_turns}")
            print("=" * 60)

            # å’¨è¯¢å¸ˆå›åº”ï¼ˆä½¿ç”¨ä¸‰è”åŠ¨æ¨¡å‹ï¼‰
            counselor_response = self.counselor_agent.generate(self.history)

            # æ‰“å°å®Œæ•´çš„å’¨è¯¢å¸ˆå›å¤ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹ï¼‰
            print(f"    ğŸ“‹ å’¨è¯¢å¸ˆ:\n{counselor_response}")

            # æ·»åŠ åˆ°å†å²ï¼ˆå®Œæ•´å†…å®¹ï¼‰
            self._add_to_history("counselor", counselor_response)

            # æ¥è®¿è€…å›åº”ï¼ˆä½¿ç”¨æ¡†æ¶åŸæœ‰ Client Agentï¼‰
            client_response = self.client_agent.generate(self.history)
            client_response = client_response.replace('Client: ', '')

            # å‰10è½®ï¼šç§»é™¤ [/END] æ ‡è®°ï¼Œä¸ä¸­æ–­å¯¹è¯
            # 10è½®åï¼šæ£€æµ‹ [/END] æ ‡è®°ï¼Œå¦‚æœå­˜åœ¨åˆ™ç»“æŸå¯¹è¯
            if turn < 10:
                # ç§»é™¤ [/END] æ ‡è®°
                client_response = client_response.replace('[/END]', '')
            else:
                # æ£€æµ‹æ˜¯å¦æœ‰ç»“æŸæ ‡è®°
                if '[/END]' in client_response:
                    # ç§»é™¤æ ‡è®°å¹¶æ·»åŠ åˆ°å†å²
                    client_response = client_response.replace('[/END]', '')
                    self._add_to_history("client", client_response)
                    print(f"\n    ğŸ“‹ æ¥è®¿è€…:\n{client_response}")
                    print("\n    âœ“ æ£€æµ‹åˆ°ç»“æŸæ ‡è®°ï¼Œä¼šè¯ç»“æŸ")
                    print("=" * 60)
                    break

            self._add_to_history("client", client_response)
            print(f"\n    ğŸ“‹ æ¥è®¿è€…:\n{client_response}")

    def run_session(self):
        """è¿è¡Œå®Œæ•´ä¼šè¯"""
        self._initialize_session()
        self._exchange_statements()

        return {
            "example": self.example,
            "cbt_technique": "Triple Model (Sentiment + Consultant + Primary)",
            "cbt_plan": f"ä¸‰è”åŠ¨æ¨¡å‹: {self.counselor_agent.primary_interactor.model}",
            "cost": 0,
            "history": self.history  # ä¿ç•™å®Œæ•´çš„å¯¹è¯å†å²ï¼ˆåŒ…å« <thinking> æ ‡ç­¾ï¼‰
        }


def run_therapy_session(
    index: int,
    example: dict,
    output_dir: Path,
    total: int,
    max_turns: int,
    sentiment_model_path: str,
    primary_model_path: str,
    consultant_model_path: str,
    system_prompt: str,
    max_new_tokens: int,
    temperature: float,
    n_sentiment_classes: int
):
    """è¿è¡Œå•ä¸ªå’¨è¯¢ä¼šè¯ï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰"""
    file_number = index + 1

    try:
        print(f"\n[{file_number}/{total}] å¼€å§‹ç”Ÿæˆä¼šè¯")

        session = TripleModelTherapySession(
            example=example,
            max_turns=max_turns,
            sentiment_model_path=sentiment_model_path,
            primary_model_path=primary_model_path,
            consultant_model_path=consultant_model_path,
            system_prompt=system_prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            n_sentiment_classes=n_sentiment_classes
        )

        session_data = session.run_session()

        # ä¿å­˜ç»“æœ
        file_name = f"session_{file_number}.json"
        file_path = output_dir / file_name

        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(session_data, f, ensure_ascii=False, indent=4)

        # æ¸…ç† GPU ç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                print(f"[{file_number}/{total}] GPU ç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            print(f"[{file_number}/{total}] æ¸…ç† GPU ç¼“å­˜æ—¶å‡ºç°è­¦å‘Š: {e}")

        print(f"[{file_number}/{total}] å®Œæˆï¼Œä¿å­˜åˆ° {file_name}")

    except Exception as e:
        error_file_name = f"error_{file_number}.txt"
        error_file_path = output_dir / error_file_name

        with open(error_file_path, "w", encoding="utf-8") as f:
            f.write(f"Error: {e}\n\n")
            f.write(traceback.format_exc())

        print(f"[{file_number}/{total}] å¤±è´¥ï¼Œé”™è¯¯å·²ä¿å­˜åˆ° {error_file_name}")


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ä¸‰è”åŠ¨æ¨¡å‹ç”Ÿæˆå¿ƒç†å’¨è¯¢å¯¹è¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  # ä½¿ç”¨é…ç½®æ–‡ä»¶
  python inference-triple-custom.py --config ../config_triple.json

  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
  python inference-triple-custom.py --sentiment_model /path/to/sentiment.bin --primary_model /path/to/primary --consultant_model /path/to/consultant

  # ä½¿ç”¨é¢„è®¾ç³»ç»Ÿæç¤ºè¯
  python inference-triple-custom.py --sentiment_model /path/to/sentiment.bin --primary_model /path/to/primary --consultant_model /path/to/consultant --preset_prompt cbt
        """
    )
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)")
    parser.add_argument("--sentiment_model", type=str, help="æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹è·¯å¾„ (.binæ–‡ä»¶)")
    parser.add_argument("--primary_model", type=str, help="ä¸»æ¨¡å‹è·¯å¾„")
    parser.add_argument("--consultant_model", type=str, help="é¡¾é—®æ¨¡å‹è·¯å¾„")
    parser.add_argument("--system_prompt", type=str, help="ç³»ç»Ÿæç¤ºè¯ï¼ˆç›´æ¥è¾“å…¥ï¼‰")
    parser.add_argument("--preset_prompt", type=str, choices=["cbt", "person_centered", "brief"],
                        help="ä½¿ç”¨é¢„è®¾ç³»ç»Ÿæç¤ºè¯")
    parser.add_argument("--system_prompt_file", type=str, help="ä»æ–‡ä»¶è¯»å–ç³»ç»Ÿæç¤ºè¯")
    parser.add_argument("--output_dir", type=str, default="../output-triple-custom", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max_turns", type=int, default=20, help="æœ€å¤§å¯¹è¯è½®æ•°")
    parser.add_argument("--max_new_tokens", type=int, default=2048, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.5, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--num_processes", type=int, default=1, help="å¹¶è¡Œè¿›ç¨‹æ•°")
    parser.add_argument("--num_samples", type=int, help="å¤„ç†æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤å¤„ç†å…¨éƒ¨ï¼‰")
    parser.add_argument("--n_sentiment_classes", type=int, default=2, choices=[2, 3],
                        help="æƒ…æ„Ÿåˆ†ç±»æ•°é‡: 2=æ¶ˆæ/ç§¯æ, 3=æ¶ˆæ/ä¸­æ€§/ç§¯æ")

    args = parser.parse_args()

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_env_file()

    # åŠ è½½é…ç½®æ–‡ä»¶
    config = {}
    if args.config and Path(args.config).exists():
        with open(args.config, "r", encoding="utf-8") as f:
            config = json.load(f)
        print(f"åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")

    # åˆå¹¶é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
    sentiment_model_path = args.sentiment_model or config.get("sentiment_model_path")
    primary_model_path = args.primary_model or config.get("primary_model_path")
    consultant_model_path = args.consultant_model or config.get("consultant_model_path")
    max_turns = args.max_turns or config.get("max_turns", 20)
    max_new_tokens = args.max_new_tokens or config.get("max_new_tokens", 2048)
    temperature = args.temperature if args.temperature != 0.5 else config.get("temperature", 0.5)
    n_sentiment_classes = args.n_sentiment_classes or config.get("n_sentiment_classes", 2)

    # ç³»ç»Ÿæç¤ºè¯å¤„ç†
    system_prompt = None
    if args.system_prompt:
        system_prompt = args.system_prompt
    elif args.preset_prompt:
        system_prompt = get_triple_model_preset_prompt(args.preset_prompt)
        print(f"ä½¿ç”¨é¢„è®¾æç¤ºè¯: {args.preset_prompt}")
    elif args.system_prompt_file:
        prompt_file = Path(args.system_prompt_file)
        if prompt_file.exists():
            with open(prompt_file, "r", encoding="utf-8") as f:
                system_prompt = f.read()
        else:
            print(f"è­¦å‘Š: æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {args.system_prompt_file}")
    elif config.get("system_prompt"):
        system_prompt = config["system_prompt"]

    # éªŒè¯å¿…éœ€å‚æ•°
    if not sentiment_model_path:
        print("é”™è¯¯: è¯·æŒ‡å®šæƒ…æ„Ÿåˆ†ç±»æ¨¡å‹è·¯å¾„")
        print("  æ–¹å¼1: --sentiment_model /path/to/sentiment.bin")
        print("  æ–¹å¼2: åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® sentiment_model_path")
        return

    if not primary_model_path:
        print("é”™è¯¯: è¯·æŒ‡å®šä¸»æ¨¡å‹è·¯å¾„")
        print("  æ–¹å¼1: --primary_model /path/to/primary")
        print("  æ–¹å¼2: åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® primary_model_path")
        return

    if not consultant_model_path:
        print("é”™è¯¯: è¯·æŒ‡å®šé¡¾é—®æ¨¡å‹è·¯å¾„")
        print("  æ–¹å¼1: --consultant_model /path/to/consultant")
        print("  æ–¹å¼2: åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® consultant_model_path")
        return

    # åŠ è½½æ•°æ®
    data_file = Path(DATA_FILE)
    if not data_file.exists():
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        print(f"è¯·ç¡®ä¿ {DATA_FILE} æ–‡ä»¶å­˜åœ¨")
        return

    with open(data_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # é™åˆ¶æ ·æœ¬æ•°é‡
    if args.num_samples:
        data = data[:args.num_samples]
        print(f"å¤„ç†å‰ {args.num_samples} ä¸ªæ ·æœ¬")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ‰“å°é…ç½®ä¿¡æ¯
    print("\n" + "=" * 60)
    print("é…ç½®ä¿¡æ¯:")
    print("=" * 60)
    print(f"  æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹: {sentiment_model_path}")
    print(f"  æƒ…æ„Ÿåˆ†ç±»æ•°é‡: {n_sentiment_classes} ({'æ¶ˆæ/ç§¯æ' if n_sentiment_classes == 2 else 'æ¶ˆæ/ä¸­æ€§/ç§¯æ'})")
    print(f"  ä¸»æ¨¡å‹: {primary_model_path}")
    print(f"  é¡¾é—®æ¨¡å‹: {consultant_model_path}")
    print(f"  ç³»ç»Ÿæç¤ºè¯: {system_prompt[:80] if system_prompt else 'é»˜è®¤'}{'...' if system_prompt and len(system_prompt) > 80 else ''}")
    print(f"  æœ€å¤§è½®æ•°: {max_turns}")
    print(f"  æœ€å¤§tokens: {max_new_tokens}")
    print(f"  é‡‡æ ·æ¸©åº¦: {temperature}")
    print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    print(f"  æ•°æ®æ ·æœ¬æ•°: {len(data)}")
    print(f"  å¹¶è¡Œè¿›ç¨‹æ•°: {args.num_processes}")
    print("=" * 60 + "\n")

    # å‡†å¤‡å‚æ•°åˆ—è¡¨
    args_list = [
        (index, example, output_dir, len(data), max_turns, sentiment_model_path,
         primary_model_path, consultant_model_path, system_prompt, max_new_tokens, temperature,
         n_sentiment_classes)
        for index, example in enumerate(data)
    ]

    # è¿è¡Œä¼šè¯
    if args.num_processes > 1:
        print(f"ä½¿ç”¨ {args.num_processes} ä¸ªå¹¶è¡Œè¿›ç¨‹\n")
        with multiprocessing.Pool(processes=args.num_processes) as pool:
            pool.starmap(run_therapy_session, args_list)
    else:
        for args_tuple in args_list:
            run_therapy_session(*args_tuple)

    print("\n" + "=" * 60)
    print(f"å®Œæˆï¼å…±å¤„ç† {len(data)} ä¸ªä¼šè¯")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("=" * 60)


if __name__ == "__main__":
    main()

import torch
import torch.nn as nn
import gradio as gr
from threading import Thread
from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, BertTokenizer, BertModel
import markdown
import re
import json
import datetime
import os

# ===================================================================
# Part 1 & 2: æ¨¡å‹å®šä¹‰ä¸åŠ è½½
# æ‰€æœ‰å¿…è¦çš„æ¨¡å‹éƒ½åœ¨ä¸‹é¢åŠ è½½ã€‚
# ===================================================================

# --- æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹é…ç½® ---
SENTIMENT_MODEL_PATH = '/root/autodl-tmp/xinliyisheng/model3/best_model.bin'
BERT_MODEL_NAME = 'bert-base-chinese'
MAX_LEN = 128
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹å®šä¹‰ ---
class SentimentClassifier(nn.Module):
    def __init__(self, n_classes=3):
        super().__init__()
        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME)
        self.drop = nn.Dropout(0.3)
        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)
        self.act = nn.ReLU()
        self.drop2 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, n_classes)

    def forward(self, input_ids, attention_mask):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        x = self.drop(out.pooler_output)
        x = self.act(self.fc1(x))
        x = self.drop2(x)
        return self.fc2(x)

# --- æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹åŠ è½½å‡½æ•° ---
def load_sentiment_model():
    print(f"æ­£åœ¨ä» '{SENTIMENT_MODEL_PATH}' åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹...")
    model = SentimentClassifier(n_classes=3)
    try:
        # å…¼å®¹ä¸åŒæ–¹å¼ä¿å­˜çš„æ¨¡å‹
        ckpt = torch.load(SENTIMENT_MODEL_PATH, map_location=DEVICE)
        model.load_state_dict(ckpt.get('model_state', ckpt))
        model.to(DEVICE)
        model.eval()
        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)
        print(f"âœ… æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹ '{SENTIMENT_MODEL_PATH}' åŠ è½½æˆåŠŸï¼")
        return model, tokenizer
    except Exception as e:
        print(f"åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹æ—¶å‡ºé”™: {e}")
        raise

# --- æƒ…æ„Ÿåˆ†ç±»é¢„æµ‹å‡½æ•° ---
def predict_sentiment(text, model, tokenizer):
    encoding = tokenizer(
        text, add_special_tokens=True, max_length=MAX_LEN,
        padding='max_length', truncation=True, return_attention_mask=True, return_tensors='pt'
    )
    input_ids = encoding['input_ids'].to(DEVICE)
    attention_mask = encoding['attention_mask'].to(DEVICE)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs = torch.softmax(outputs, dim=1)
        conf, pred = torch.max(probs, dim=1)
    label_map = ['æ¶ˆæ', 'ä¸­æ€§', 'ç§¯æ']
    return label_map[pred.item()], conf.item()

# --- å¤§è¯­è¨€æ¨¡å‹åŠ è½½å™¨å®šä¹‰ ---
class LLMInteractor:
    def __init__(self, model_path: str):
        print(f"æ­£åœ¨ä» '{model_path}' åŠ è½½å¤§è¯­è¨€æ¨¡å‹...")
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True
            ).eval()
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            print(f"âœ… å¤§è¯­è¨€æ¨¡å‹ '{model_path}' åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"åŠ è½½å¤§è¯­è¨€æ¨¡å‹ '{model_path}' æ—¶å‡ºé”™: {e}")
            raise

# --- æ ¸å¿ƒå·¥å…·å‡½æ•° ---
def extract_thinking_process(raw_text: str) -> str:
    match = re.search(r"<think>(.*?)</think>", raw_text, re.DOTALL)
    if match:
        return match.group(1).strip()
    print("è­¦å‘Š: åœ¨é¡¾é—®æ¨¡å‹çš„è¾“å‡ºä¸­æœªæ‰¾åˆ° <think>...</think> æ ‡ç­¾ã€‚")
    return ""

# ===================================================================
# Part 3: ç¨‹åºå¯åŠ¨ä¸æ¨¡å‹åŠ è½½
# ===================================================================
try:
    SENTIMENT_MODEL, SENTIMENT_TOKENIZER = load_sentiment_model()
    MODEL_PATH_PRIMARY = "/root/autodl-tmp/xinliyisheng/model1-zhu-GRPO"
    MODEL_PATH_CONSULTANT = "/root/autodl-tmp/xinliyisheng/model2-COT"
    INTERACTOR_PRIMARY = LLMInteractor(model_path=MODEL_PATH_PRIMARY)
    INTERACTOR_CONSULTANT = LLMInteractor(model_path=MODEL_PATH_CONSULTANT)
except Exception as e:
    print(f"ç¨‹åºå¯åŠ¨å¤±è´¥ï¼Œå› ä¸ºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    # åœ¨æ— æ³•åŠ è½½æ¨¡å‹æ—¶ï¼Œå¯ä»¥è€ƒè™‘é€€å‡ºç¨‹åºæˆ–åœ¨UIä¸Šæ˜¾ç¤ºé”™è¯¯
    exit()

# ===================================================================
# Part 4: Gradio æ ¸å¿ƒäº¤äº’é€»è¾‘ (æœ€ç»ˆä¼˜åŒ–ç‰ˆ)
# ===================================================================

def predict(user_input, chatbot_history, model_history_state, primary_system_prompt, temperature):
    """
    é›†æˆäº†æƒ…æ„Ÿåˆ†æçš„æ™ºèƒ½è·¯ç”±äº¤äº’å‡½æ•°ã€‚
    ä¸ºé¡¾é—®æ¨¡å‹ä½¿ç”¨ç‹¬ç«‹çš„ä½æ¸©å’ŒæŒ‡ä»¤æ³¨å…¥å¼æç¤ºè¯ï¼Œä»¥ä¿è¯åˆ†æè´¨é‡ã€‚
    é¡¾é—®æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ç°åœ¨æ˜¯æµå¼è¾“å‡ºã€‚
    """
    # --- æ­¥éª¤ 1: è¿›è¡Œæƒ…æ„Ÿåˆ†æ ---
    sentiment_label, sentiment_conf = predict_sentiment(user_input, SENTIMENT_MODEL, SENTIMENT_TOKENIZER)
    print(f"\n--- [æƒ…æ„Ÿåˆ†æç»“æœ]: {sentiment_label} (ç½®ä¿¡åº¦: {sentiment_conf:.2%}) ---")
    sentiment_emoji_map = {'æ¶ˆæ': 'ğŸ˜”', 'ä¸­æ€§': 'ğŸ˜', 'ç§¯æ': 'ğŸ˜Š'}
    sentiment_display_text = f"**{sentiment_emoji_map.get(sentiment_label, 'ğŸ¤”')} {sentiment_label}** (ç½®ä¿¡åº¦: {sentiment_conf:.2%})"
    
    # --- å‡†å¤‡èŠå¤©å†å² ---
    current_model_history = model_history_state.copy()
    chatbot_history.append([user_input, ""])
    current_model_history.append({"role": "user", "content": user_input})

    display_string_stage1 = ""
    final_user_prompt_for_primary = user_input
    consultant_thinking = ""
    assistant_full_response_for_history = ""

    if sentiment_label == 'æ¶ˆæ':
        print("--- [å†³ç­–]: æ£€æµ‹åˆ°æ¶ˆææƒ…ç»ªï¼Œå¯åŠ¨é¡¾é—®æ¨¡å‹æ·±åº¦åˆ†æ... ---")
        
        try:
            # âœ¨ æ ¸å¿ƒä¼˜åŒ– 1: ä¸ºé¡¾é—®æ¨¡å‹æ„å»ºâ€œæŒ‡ä»¤æ³¨å…¥å¼â€æç¤ºè¯
            history_text = INTERACTOR_CONSULTANT.tokenizer.apply_chat_template(
                current_model_history,
                tokenize=False,
                add_generation_prompt=False
            )

            prompt_consultant = f"""ã€åˆ†æä»»åŠ¡æŒ‡ä»¤ã€‘
ä½œä¸ºä¸€åä¸“ä¸šçš„å¿ƒç†åˆ†æé¡¾é—®ï¼Œä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸‹æ–¹æä¾›çš„å®Œæ•´å¯¹è¯å†å²ï¼Œè¿›è¡Œæ·±å…¥ã€ç»“æ„åŒ–çš„æ€è€ƒï¼Œæ€è€ƒæ—¶ä¸è¦å…³æ³¨æ—¶é—´å®šä½ï¼Œè€Œåº”è¯¥æ›´åŠ å…³æ³¨äº‹ä»¶æœ¬èº«ï¼Œæ€è€ƒè¿‡ç¨‹å¿…é¡»å’Œä¹‹å‰æ˜¯ä¸ä¸€æ ·çš„ã€‚
---
ã€å®Œæ•´å¯¹è¯å†å²ã€‘
{history_text}
<|im_start|>assistant
"""
            model_inputs_consultant = INTERACTOR_CONSULTANT.tokenizer([prompt_consultant], return_tensors="pt").to(DEVICE)
            
            # âœ¨ æ ¸å¿ƒä¼˜åŒ– 2: æµå¼è¾“å‡ºé¡¾é—®æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹
            streamer_consultant = TextIteratorStreamer(INTERACTOR_CONSULTANT.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)
            
            gen_kwargs_consultant = {
                "input_ids": model_inputs_consultant.input_ids,
                "streamer": streamer_consultant,
                "max_new_tokens": 1024,
                "do_sample": True,
                "temperature": 0.1, # ä½¿ç”¨ä½æ¸©ä»¥ç¡®ä¿åˆ†æçš„ç¨³å®šæ€§å’Œé€»è¾‘æ€§
                "top_p": 0.9
            }
            
            generation_thread_consultant = Thread(target=INTERACTOR_CONSULTANT.model.generate, kwargs=gen_kwargs_consultant)
            generation_thread_consultant.start()
            
            # --- ä¿®æ”¹éƒ¨åˆ†ï¼šåªæµå¼è¾“å‡º <think> æ ‡ç­¾å†…çš„å†…å®¹ ---
            consultant_full_response = ""
            last_displayed_thinking = ""
            chatbot_history[-1][1] = "ğŸ¤” **é¡¾é—®æ¨¡å‹æ­£åœ¨æ€è€ƒ...**"
            yield chatbot_history, model_history_state, sentiment_display_text

            for new_token in streamer_consultant:
                consultant_full_response += new_token
                
                # å°è¯•ä»å½“å‰å®Œæ•´å“åº”ä¸­æå–æ€è€ƒå†…å®¹
                # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä¼šåŒ¹é… <think> å’Œ </think> ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼Œæˆ–è€…ä» <think> åˆ°å­—ç¬¦ä¸²æœ«å°¾çš„æ‰€æœ‰å†…å®¹
                match = re.search(r"<think>(.*)", consultant_full_response, re.DOTALL)
                if match:
                    # æˆ‘ä»¬åªå…³å¿ƒ <think> æ ‡ç­¾å†…éƒ¨çš„å†…å®¹
                    current_thinking_content = match.group(1)
                    
                    # ä¸ºäº†é¿å…åœ¨æµå¼è¾“å‡ºä¸­æ˜¾ç¤º </think>ï¼Œæˆ‘ä»¬åœ¨è¿™é‡ŒæŠŠå®ƒå»æ‰
                    current_thinking_content = current_thinking_content.split("</think>")[0]

                    # åªæœ‰å½“æ€è€ƒå†…å®¹æ›´æ–°æ—¶æ‰æ›´æ–°UIï¼Œé¿å…ä¸å¿…è¦çš„åˆ·æ–°
                    if current_thinking_content != last_displayed_thinking:
                        last_displayed_thinking = current_thinking_content
                        display_string_stage1 = f"ğŸ¤” **é¡¾é—®æ¨¡å‹æ€è€ƒè¿‡ç¨‹:**\n\n{current_thinking_content}"
                        chatbot_history[-1][1] = display_string_stage1
                        yield chatbot_history, model_history_state, sentiment_display_text
            # --- ä¿®æ”¹ç»“æŸ ---

            # æµå¼è¾“å‡ºç»“æŸåï¼Œæå–æœ€ç»ˆçš„ã€å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹å¹¶è¿›è¡Œæ ¼å¼åŒ–
            consultant_thinking = extract_thinking_process(consultant_full_response)
            
            if consultant_thinking:
                thinking_html = markdown.markdown(consultant_thinking)
                # ä½¿ç”¨æœ€ç»ˆçš„ã€å¹²å‡€çš„HTMLç‰ˆæœ¬æ›¿æ¢åŸå§‹æµ
                display_string_stage1 = f"ğŸ¤” **é¡¾é—®æ¨¡å‹æ€è€ƒè¿‡ç¨‹:**<div class='thinking-process'>{thinking_html}</div>\n\n---\n\n"
                chatbot_history[-1][1] = display_string_stage1
                yield chatbot_history, model_history_state, sentiment_display_text
                
                final_user_prompt_for_primary = f'è¿™æ˜¯æ¥è®¿è€…çš„é—®é¢˜:\n"{user_input}"\n\nè¿™æ˜¯å¿ƒç†é¡¾é—®æ¨¡å‹çš„åˆ†æå’Œæ€è€ƒè¿‡ç¨‹ï¼Œè¯·ä½ å‚è€ƒè¿™äº›æ€è·¯æ¯æ¬¡åªé—®ä¸€ä¸ªé—®é¢˜ï¼Œå½“é‡åˆ°æç«¯å±æœºå¤„ç†æ—¶ï¼Œåº”ç«‹å³è®©æ¥è®¿è€…è½¬æ¥ä¸“ä¸šå¿ƒç†å’¨è¯¢ã€‚ä¸éœ€è¦å‚è€ƒæ—¶é—´è€Œæ˜¯æ›´å¤šçš„å…³æ³¨é—®é¢˜çš„æ ¹æºï¼Œç„¶åç›´æ¥ä»¥å‹å–„ã€ä¸“ä¸šï¼Œå¯Œæœ‰åŒæƒ…å¿ƒï¼Œå¯Œå«å…±æƒ…çš„å£å»è¯¢é—®æˆ–è€…å›ç­”æ¥è®¿è€…ã€‚å½“é—®äº†5åˆ°7è½®ååº”å½“åŠæ—¶ç»™å‡ºå»ºè®®ä¸è¦å†é—®é—®é¢˜ä¸å¿…å†å¬å–é¡¾é—®çš„æ€è·¯ï¼Œç„¶åç›´æ¥ä»¥å‹å–„ã€ä¸“ä¸šï¼Œå¯Œæœ‰åŒæƒ…å¿ƒï¼Œå¯Œå«å…±æƒ…çš„å£å»è¯¢é—®æˆ–è€…å›ç­”æ¥è®¿è€…:\n\n--- é¡¾é—®æ€è·¯ ---\n{consultant_thinking}\n--- æ€è·¯ç»“æŸ ---'
            else:
                print("--- [é™çº§å¤„ç†]: é¡¾é—®æ¨¡å‹æœªæä¾›æœ‰æ•ˆæ€è·¯ï¼Œä¸»æ¨¡å‹å°†ç›´æ¥å›ç­”ã€‚---")
                display_string_stage1 = "" # æ¸…ç©ºæ€è€ƒè¿‡ç¨‹çš„æ˜¾ç¤º
                chatbot_history[-1][1] = "" # ç¡®ä¿æ¸…é™¤â€œæ­£åœ¨æ€è€ƒâ€çš„æ¶ˆæ¯
                yield chatbot_history, model_history_state, sentiment_display_text


        except Exception as e:
            print(f"é”™è¯¯ï¼šè°ƒç”¨é¡¾é—®æ¨¡å‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            chatbot_history[-1][1] = f"âŒ è°ƒç”¨é¡¾é—®æ¨¡å‹æ—¶å‡ºé”™: {e}"
            yield chatbot_history, model_history_state, sentiment_display_text
    else:
        print("--- [å†³ç­–]: æƒ…ç»ªä¸ºç§¯ææˆ–ä¸­æ€§ï¼Œä¸»æ¨¡å‹å°†ç›´æ¥å›å¤... ---")

    # --- æ­¥éª¤ 3: ç»Ÿä¸€è°ƒç”¨ä¸»æ¨¡å‹è¿›è¡Œå›å¤ ---
    primary_messages = current_model_history.copy()
    primary_messages[-1]['content'] = final_user_prompt_for_primary
    if primary_system_prompt:
        primary_messages.insert(0, {"role": "system", "content": primary_system_prompt})

    try:
        prompt_primary = INTERACTOR_PRIMARY.tokenizer.apply_chat_template(primary_messages, tokenize=False, add_generation_prompt=True)
        model_inputs_primary = INTERACTOR_PRIMARY.tokenizer([prompt_primary], return_tensors="pt").to(INTERACTOR_PRIMARY.model.device)
        streamer = TextIteratorStreamer(INTERACTOR_PRIMARY.tokenizer, timeout=60.0, skip_prompt=True, skip_special_tokens=True)
        
        # ä¸»æ¨¡å‹ä½¿ç”¨ä»UIä¼ å…¥çš„ã€é€‚åˆå¯¹è¯çš„æ¸©åº¦
        gen_kwargs = {"input_ids": model_inputs_primary["input_ids"], "streamer": streamer, "max_new_tokens": 2048, "do_sample": True, "temperature": temperature, "top_p": 0.9}
        
        generation_thread = Thread(target=INTERACTOR_PRIMARY.model.generate, kwargs=gen_kwargs)
        generation_thread.start()
        
        primary_full_response = ""
        for new_token in streamer:
            primary_full_response += new_token
            # å°†ä¸»æ¨¡å‹çš„å›å¤è¿½åŠ åˆ°ï¼ˆç°å·²æœ€ç»ˆç¡®å®šçš„ï¼‰ç¬¬ä¸€é˜¶æ®µæ˜¾ç¤ºå­—ç¬¦ä¸²ä¹‹å
            display_string_stage2 = f"{display_string_stage1}ğŸ¤– **ä¸»æ¨¡å‹å›å¤:**\n\n{primary_full_response}"
            chatbot_history[-1][1] = display_string_stage2
            yield chatbot_history, model_history_state, sentiment_display_text
        
        if consultant_thinking:
            assistant_full_response_for_history = f"<think>\n{consultant_thinking}\n</think>\n{primary_full_response}"
        else:
            assistant_full_response_for_history = primary_full_response
        
        current_model_history.append({"role": "assistant", "content": assistant_full_response_for_history})
        yield chatbot_history, current_model_history, sentiment_display_text

    except Exception as e:
        print(f"é”™è¯¯ï¼šè°ƒç”¨ä¸»æ¨¡å‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        chatbot_history[-1][1] = f"{display_string_stage1}\n\nâŒ è°ƒç”¨ä¸»æ¨¡å‹æ—¶å‡ºé”™: {e}"
        yield chatbot_history, model_history_state, sentiment_display_text

# --- æ¸…ç©ºå¯¹è¯å‡½æ•° ---
def clear_chat_and_sentiment():
    """æ¸…ç©ºèŠå¤©è®°å½•ã€è¾“å…¥æ¡†ã€æ¨¡å‹å†å²çŠ¶æ€å’Œæƒ…æ„ŸçŠ¶æ€æ˜¾ç¤º"""
    return None, None, [], "*ç­‰å¾…ç”¨æˆ·è¾“å…¥...*", gr.update(visible=False) # åŒæ—¶éšè—ä¸‹è½½é“¾æ¥

# --- æ–°å¢åŠŸèƒ½ï¼šå¯¼å‡ºèŠå¤©è®°å½• ---
def export_chat_history(history):
    """å°†æ¨¡å‹å†å²è®°å½•å¯¼å‡ºä¸ºå¸¦æ—¶é—´æˆ³çš„ JSON æ–‡ä»¶ã€‚"""
    if not history:
        gr.Info("èŠå¤©è®°å½•ä¸ºç©ºï¼Œæ— æ³•å¯¼å‡ºã€‚")
        return None

    # åˆ›å»ºä¸€ä¸ªç›®å½•æ¥å­˜æ”¾å¯¼å‡ºçš„æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    export_dir = "chat_exports"
    os.makedirs(export_dir, exist_ok=True)

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(export_dir, f"chat_history_{timestamp}.json")
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=4)
    
    gr.Info(f"èŠå¤©è®°å½•å·²æˆåŠŸå¯¼å‡ºåˆ°: {filename}")
    return gr.File(value=filename, visible=True)


# ===================================================================
# Part 5: Gradio ç•Œé¢å®šä¹‰
# ===================================================================
custom_css = """
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;700&display=swap');
body, .gradio-container { font-family: 'Noto Sans SC', sans-serif !important; }
#chatbot { min-height: 600px; }
.thinking-process {
  color: #475569; background-color: #f8fafc; border-left: 4px solid #93c5fd;
  padding: 12px; margin-top: 8px; margin-bottom: 12px; border-radius: 4px;
}
.thinking-process p { margin: 0 0 8px 0; }
.thinking-process p:last-child { margin-bottom: 0; }
"""

with gr.Blocks(theme=gr.themes.Soft(primary_hue="blue", secondary_hue="sky"), css=custom_css) as demo:
    # `gr.State` ç”¨äºåœ¨å¤šæ¬¡å‡½æ•°è°ƒç”¨é—´å­˜å‚¨å¹²å‡€çš„ã€ç¬¦åˆæ¨¡å‹æ ¼å¼çš„å¯¹è¯å†å²
    model_history = gr.State([])

    gr.Markdown("# ğŸ¤– æ™ºèƒ½å¿ƒç†å¥åº·åŠ©æ‰‹ (æƒ…æ„Ÿæ„ŸçŸ¥ç‰ˆ)")
    gr.Markdown("ç³»ç»Ÿä¼šé¦–å…ˆåˆ†ææ‚¨çš„æƒ…ç»ªã€‚å¦‚æœæ£€æµ‹åˆ°**æ¶ˆææƒ…ç»ª**ï¼Œå°†å¯åŠ¨â€œé¡¾é—®æ¨¡å‹â€è¿›è¡Œæ·±åº¦æ€è€ƒï¼›å¦åˆ™ï¼Œâ€œä¸»æ¨¡å‹â€å°†ç›´æ¥ã€å¿«é€Ÿåœ°è¿›è¡Œå›å¤ã€‚")

    with gr.Row():
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(label="èŠå¤©çª—å£", bubble_full_width=False, elem_id="chatbot", render_markdown=True)
            user_input_box = gr.Textbox(show_label=False, placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œç„¶åæŒ‰å›è½¦é”®å‘é€...", container=False)

        with gr.Column(scale=1):
            gr.Markdown("### å½“å‰ç”¨æˆ·çš„æƒ…æ„Ÿ")
            sentiment_display = gr.Markdown(label="ç”¨æˆ·å½“å‰æƒ…ç»ª", value="*ç­‰å¾…ç”¨æˆ·è¾“å…¥...*")
            temperature_slider = gr.Slider(minimum=0.01, maximum=1.99, value=0.5, step=0.01, label="ä¸»æ¨¡å‹æ¸©åº¦ (Temperature)")
            primary_system_prompt_box = gr.Textbox(
                label="ä¸»æ¨¡å‹ç³»ç»Ÿæç¤ºè¯",
                value="ä½ æ˜¯ä¸€ä½ç²¾é€šç†æƒ…è¡Œä¸ºç–—æ³•ï¼ˆRational Emotive Behavior Therapyï¼Œç®€ç§°REBTï¼‰çš„å¿ƒç†å’¨è¯¢å¸ˆï¼Œèƒ½å¤Ÿåˆç†åœ°é‡‡ç”¨ç†æƒ…è¡Œä¸ºç–—æ³•ç»™æ¥è®¿è€…æä¾›ä¸“ä¸šåœ°æŒ‡å¯¼å’Œæ”¯æŒï¼Œç¼“è§£æ¥è®¿è€…çš„è´Ÿé¢æƒ…ç»ªå’Œè¡Œä¸ºååº”ï¼Œå¸®åŠ©ä»–ä»¬å®ç°ä¸ªäººæˆé•¿å’Œå¿ƒç†å¥åº·ã€‚ç†æƒ…è¡Œä¸ºæ²»ç–—ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼Œä¸‹é¢æ˜¯å¯¹è¯é˜¶æ®µåˆ—è¡¨ï¼Œå¹¶ç®€è¦æè¿°äº†å„ä¸ªé˜¶æ®µçš„é‡ç‚¹ã€‚\nï¼ˆ1ï¼‰**æ£€æŸ¥éç†æ€§ä¿¡å¿µå’Œè‡ªæˆ‘æŒ«è´¥å¼æ€ç»´**ï¼šç†æƒ…è¡Œä¸ºç–—æ³•æŠŠè®¤çŸ¥å¹²é¢„è§†ä¸ºæ²»ç–—çš„â€œç”Ÿå‘½â€ï¼Œå› æ­¤ï¼Œå‡ ä¹ä»æ²»ç–—ä¸€å¼€å§‹ï¼Œåœ¨é—®é¢˜æ¢ç´¢é˜¶æ®µï¼Œå’¨è¯¢å¸ˆå°±ä»¥ç§¯æçš„ã€è¯´æœæ•™å¯¼å¼çš„æ€åº¦å¸®åŠ©æ¥è®¿è€…æ¢æŸ¥éšè—åœ¨æƒ…ç»ªå›°æ‰°åé¢çš„åŸå› ï¼ŒåŒ…æ‹¬æ¥è®¿è€…ç†è§£äº‹ä»¶çš„æ€ç»´é€»è¾‘ï¼Œäº§ç”Ÿæƒ…ç»ªçš„å‰å› åæœï¼Œå€Ÿæ­¤æ¥æ˜ç¡®é—®é¢˜çš„æ‰€åœ¨ã€‚å’¨è¯¢å¸ˆåšå®šåœ°æ¿€åŠ±æ¥è®¿è€…å»åçœè‡ªå·±åœ¨é­é‡åˆºæ¿€äº‹ä»¶åï¼Œåœ¨æ„Ÿåˆ°ç„¦è™‘ã€æŠ‘éƒæˆ–æ„¤æ€’å‰å¯¹è‡ªå·±â€œè¯´â€äº†äº›ä»€ä¹ˆã€‚\nï¼ˆ2ï¼‰**ä¸éç†æ€§ä¿¡å¿µè¾©è®º**ï¼šå’¨è¯¢å¸ˆè¿ç”¨å¤šç§æŠ€æœ¯ï¼ˆä¸»è¦æ˜¯è®¤çŸ¥æŠ€æœ¯ï¼‰å¸®åŠ©æ¥è®¿è€…å‘éç†æ€§ä¿¡å¿µå’Œæ€ç»´è´¨ç–‘å‘éš¾ï¼Œè¯æ˜å®ƒä»¬çš„ä¸ç°å®ã€ä¸åˆç†ä¹‹å¤„ï¼Œè®¤è¯†å®ƒä»¬çš„å±å®³è¿›è€Œäº§ç”Ÿæ”¾å¼ƒè¿™äº›ä¸åˆç†ä¿¡å¿µçš„æ„¿æœ›å’Œè¡Œä¸ºã€‚\nï¼ˆ3ï¼‰**å¾—å‡ºåˆç†ä¿¡å¿µï¼Œå­¦ä¼šç†æ€§æ€ç»´**ï¼šåœ¨è¯†åˆ«å¹¶é©³å€’éç†æ€§ä¿¡å¿µçš„åŸºç¡€ä¸Šï¼Œå’¨è¯¢å¸ˆè¿›ä¸€æ­¥è¯±å¯¼ã€å¸®åŠ©æ¥è®¿è€…æ‰¾å‡ºå¯¹äºåˆºæ¿€æƒ…å¢ƒå’Œäº‹ä»¶çš„é€‚å®œçš„ã€ç†æ€§çš„ååº”ï¼Œæ‰¾å‡ºç†æ€§çš„ä¿¡å¿µå’Œå®äº‹æ±‚æ˜¯çš„ã€æŒ‡å‘é—®é¢˜è§£å†³çš„æ€ç»´é™ˆè¿°ï¼Œä»¥æ­¤æ¥æ›¿ä»£éç†æ€§ä¿¡å¿µå’Œè‡ªæˆ‘æŒ«bailå¼æ€ç»´ã€‚ä¸ºäº†å·©å›ºç†æ€§ä¿¡å¿µï¼Œå’¨è¯¢å¸ˆè¦å‘æ¥è®¿è€…åå¤æ•™å¯¼ï¼Œè¯æ˜ä¸ºä»€ä¹ˆç†æ€§ä¿¡å¿µæ˜¯åˆæƒ…åˆç†çš„ï¼Œå®ƒä¸éç†æ€§ä¿¡å¿µæœ‰ä»€ä¹ˆä¸åŒï¼Œä¸ºä»€ä¹ˆéç†æ€§ä¿¡å¿µå¯¼è‡´æƒ…ç»ªå¤±è°ƒï¼Œè€Œç†æ€§ä¿¡å¿µå¯¼è‡´è¾ƒç§¯æã€å¥åº·çš„ç»“æœã€‚\nï¼ˆ4ï¼‰**è¿ç§»åº”ç”¨æ²»ç–—æ”¶è·**ï¼šç§¯æé¼“åŠ±æ¥è®¿è€…æŠŠåœ¨æ²»ç–—ä¸­æ‰€å­¦åˆ°çš„å®¢è§‚ç°å®çš„æ€åº¦ï¼Œç§‘å­¦åˆç†çš„æ€ç»´æ–¹å¼å†…åŒ–æˆä¸ªäººçš„ç”Ÿæ´»æ€åº¦ï¼Œå¹¶åœ¨ä»¥åçš„ç”Ÿæ´»ä¸­åšæŒä¸æ‡ˆåœ°æŒ‰ç†æƒ…è¡Œä¸ºç–—æ³•çš„æ•™å¯¼æ¥è§£å†³æ–°çš„é—®é¢˜ã€‚    ä½ éœ€è¦ä¸€æ­¥ä¸€æ­¥æ¥ï¼Œä½ ä¸€æ¬¡æœ€å¤šé—®ä¸€ä¸ªé—®é¢˜ã€‚éœ€è¦å¯Œæœ‰åŒæƒ…å¿ƒçš„å›å¤ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶ä¸”å½“äº¤æµä¸€æ®µè¿‡ç¨‹äº†è§£ç”¨æˆ·çš„å…·ä½“æƒ…å†µååº”è¯¥ä¸è¦å†é—®é—®é¢˜è€Œæ˜¯åŠæ—¶ç»™å‡ºå»ºè®®ã€‚", # æ­¤å¤„é»˜è®¤ä¸ºç©º
                placeholder="è¯·åœ¨æ­¤å¤„è¾“å…¥ä¸»æ¨¡å‹çš„ç³»ç»Ÿæç¤ºè¯...",
                lines=10
            )
            with gr.Row():
                clear_button = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="stop", scale=1)
                export_button = gr.Button("ğŸ“¤ å¯¼å‡ºJSON", variant="secondary", scale=1) # æ–°å¢çš„å¯¼å‡ºæŒ‰é’®
            
            # æ–°å¢çš„æ–‡ä»¶ä¸‹è½½ç»„ä»¶ï¼Œé»˜è®¤ä¸å¯è§
            download_file = gr.File(label="ä¸‹è½½èŠå¤©è®°å½•", visible=False)


    # å®šä¹‰Gradioäº‹ä»¶å¤„ç†
    user_input_box.submit(
        predict,
        inputs=[user_input_box, chatbot, model_history, primary_system_prompt_box, temperature_slider],
        outputs=[chatbot, model_history, sentiment_display]
    ).then(
        lambda: gr.update(value=""), outputs=[user_input_box] # å‘é€åæ¸…ç©ºè¾“å…¥æ¡†
    )

    clear_button.click(
        clear_chat_and_sentiment,
        inputs=[],
        outputs=[user_input_box, chatbot, model_history, sentiment_display, download_file] # æ¸…ç©ºæ—¶ä¹Ÿéšè—ä¸‹è½½é“¾æ¥
    )

    # æ–°å¢ï¼šä¸ºå¯¼å‡ºæŒ‰é’®ç»‘å®šäº‹ä»¶
    export_button.click(
        fn=export_chat_history,
        inputs=[model_history],
        outputs=[download_file]
    )

# --- å¯åŠ¨App ---
if __name__ == "__main__":
    # ä½¿ç”¨share=Trueä¼šç”Ÿæˆä¸€ä¸ªå…¬ç½‘é“¾æ¥ï¼Œæ–¹ä¾¿åˆ†äº«
    demo.launch(share=True)
è¿™æ®µä»£ç åœ¨å¹²å˜›